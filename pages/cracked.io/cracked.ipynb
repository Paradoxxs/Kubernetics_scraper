{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('cracked.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS archive_urls\n",
    "            (original TEXT, mimetype TEXT, timestamp TEXT, endtimestamp TEXT, groupcount integer, uniqcount integer)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS topics\n",
    "            (forum text,url TEXT, title TEXT, author TEXT)''')\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS posts\n",
    "            (url text,title TEXT,user_id, author TEXT, body TEXT,timestamp TEXT)''')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Aston\\\\Documents\\\\GitHub\\\\web_scraper\\\\cracked.io\\\\archived.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb Cell 3\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb#W2sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Load the JSON data\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb#W2sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mC:\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mUsers\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mAston\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mDocuments\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mGitHub\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mweb_scraper\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39mcracked.io\u001b[39;49m\u001b[39m\\\u001b[39;49m\u001b[39marchived.json\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb#W2sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     json_data \u001b[39m=\u001b[39m json\u001b[39m.\u001b[39mload(file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/atj/Documents/GitHub/web_scraper/cracked.io/cracked.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Insert the data into the SQLite table\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/IPython/core/interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    284\u001b[0m     )\n\u001b[0;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Aston\\\\Documents\\\\GitHub\\\\web_scraper\\\\cracked.io\\\\archived.json'"
     ]
    }
   ],
   "source": [
    "\"\"\" import json\n",
    "\n",
    "# Load the JSON data\n",
    "with open(r'C:\\Users\\Aston\\Documents\\GitHub\\web_scraper\\cracked.io\\archived.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Insert the data into the SQLite table\n",
    "for row in json_data:\n",
    "    cursor.execute('INSERT INTO archive_urls VALUES (?, ?, ?, ?, ?, ?)', row)\n",
    "\n",
    "conn.commit() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = [ \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "    'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(c):\n",
    "    cursor = c.cursor()\n",
    "    # Create a table if it doesn't exist\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS urls (url TEXT)''')\n",
    "    c.commit()\n",
    "create_tables(conn) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_forums(url):\n",
    "    page = requests.get(url)\n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    all_links = soup.find_all(\"a\")\n",
    "    pattern = re.compile(r\".*Forum-.*\")\n",
    "\n",
    "    # Filter and print the links that match the pattern\n",
    "    for link in all_links:\n",
    "        href = link.get(\"href\")\n",
    "        if href and pattern.match(href):\n",
    "            cursor.execute('''INSERT INTO urls (url) VALUES (?)''', (href,))\n",
    "            conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_forums(\"https://cracked.io/\")\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topics(url):\n",
    "    print(f\"scraping: {url[0]}\")\n",
    "    archive_url = f\"https://cracked.io/{url[0]}\"\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(archive_url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    topics = soup.find_all(\"tr\", class_=\"inline_row\")\n",
    "    for topic in topics:\n",
    "        try:\n",
    "            topic_url = topic.find(\"span\", class_=\"subject_new\").find(\"a\").get(\"href\")\n",
    "            print(f\"url: {topic_url}\")\n",
    "            title = topic.find(\"span\", class_=\"subject_new\").text.strip()\n",
    "            print(f\"Title: {title}\")\n",
    "            author = topic.find(\"div\", class_=\"author\").text.strip().split(\" \")[0]\n",
    "            print(f\"author: {author}\")\n",
    "            # Apply to sqlite\n",
    "            cursor.execute(\"INSERT or ignore INTO topics VALUES (?,?,?,?)\",(url[0],topic_url, title, author))\n",
    "            conn.commit()\n",
    "        except:\n",
    "            continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping: Forum-Announcements\n",
      "scraping: Forum-MOTQ\n",
      "scraping: Forum-Changelogs\n",
      "scraping: Forum-Feedback-Suggestions\n",
      "scraping: Forum-Accepted-Suggestions\n",
      "scraping: Forum-Denied-Suggestions\n",
      "scraping: Forum-Upgraded-Tools\n",
      "scraping: Forum-Exclusive-Releases\n",
      "scraping: Forum-Outdated-Releases\n",
      "scraping: Forum-Request\n",
      "scraping: Forum-Non-Auth\n",
      "scraping: Forum-Forum-Support-Bugs\n",
      "scraping: Forum-Staff-support\n",
      "scraping: Forum-Solved--131\n",
      "scraping: Forum-General-Hacking\n",
      "scraping: Forum-Hacking-Tools-and-Programs\n",
      "scraping: Forum-Hacking-Tutorials\n",
      "scraping: Forum-Website-Hacking\n",
      "scraping: Forum-SQL-Injection-Attacks\n",
      "scraping: Forum-Requests-for-Hacking\n",
      "scraping: Forum-Lounge\n",
      "scraping: Forum-Introduction\n",
      "scraping: Forum-LQ-Lounge\n",
      "scraping: Forum-Gaming\n",
      "scraping: Forum-First-Person-Shooters\n",
      "scraping: Forum-Fortnite\n",
      "scraping: Forum-RPG-MMPORG-games\n",
      "scraping: Forum-League-of-Legends\n",
      "scraping: Forum-Strategy-games\n",
      "scraping: Forum-Game-discussions\n",
      "scraping: Forum-Personal\n",
      "scraping: Forum-Finance\n",
      "scraping: Forum-Innuendo\n",
      "scraping: Forum-Entertainment\n",
      "scraping: Forum-Music\n",
      "scraping: Forum-Movies-Series\n",
      "scraping: Forum-Games\n",
      "scraping: Forum-Achievements-Bragging\n",
      "scraping: Forum-News-around-the-World\n",
      "scraping: Forum-Internet-Tech\n",
      "scraping: Forum-Reallife\n",
      "scraping: Forum-Graphics\n",
      "scraping: Forum-Graphic-Resources\n",
      "scraping: Forum-Paid-Graphic-Work\n",
      "scraping: Forum-Giveaways\n",
      "scraping: Forum-Ended-Giveaways\n",
      "scraping: Forum-Challenges\n",
      "scraping: Forum-International-Lounge\n",
      "scraping: Forum-Espa%C3%B1ol\n",
      "scraping: Forum-%D8%A7%D9%84%D8%B9%D9%8E%D8%B1%D9%8E%D8%A8%D9%90%D9%8A%D9%8E%D9%91%D8%A9%E2%80%8E\n",
      "scraping: Forum-Indian\n",
      "scraping: Forum-Italiano\n",
      "scraping: Forum-Rom%C3%A2nesc\n",
      "scraping: Forum-T%C3%BCrk%C3%A7e\n",
      "scraping: Forum-Balkans\n",
      "scraping: Forum-Deutsch\n",
      "scraping: Forum-Fran%C3%A7ais\n",
      "scraping: Forum-Dutch\n",
      "scraping: Forum-Portugu%C3%AAs\n",
      "scraping: Forum-Shqip\n",
      "scraping: Forum-Magyar--319\n",
      "scraping: Forum-Cracking-Tools\n",
      "scraping: Forum-Cracking-Tutorials\n",
      "scraping: Forum-Pentesting-Help\n",
      "scraping: Forum-Cracking-Configs\n",
      "scraping: Forum-OpenBullet\n",
      "scraping: Forum-OpenBullet-2-Configs\n",
      "scraping: Forum-Silverbullet\n",
      "scraping: Forum-Sentry-MBA\n",
      "scraping: Forum-BlackBullet\n",
      "scraping: Forum-STORM\n",
      "scraping: Forum-SNIPR\n",
      "scraping: Forum-Combolists--297\n",
      "scraping: Forum-Proxies\n",
      "scraping: Forum-Tutorials-Guides-etc\n",
      "scraping: Forum-Leaked-E-Books\n",
      "scraping: Forum-Udemy-Resources\n",
      "scraping: Forum-Webmaster-Resources\n",
      "scraping: Forum-Cracked-Programs\n",
      "scraping: https://cracked.to/Forum-Official-Releases\n",
      "scraping: Forum-Youtube-Twitter-and-FB-bots\n",
      "scraping: Forum-Accounts\n",
      "scraping: Forum-Streaming--108\n",
      "scraping: Forum-Gaming--109\n",
      "scraping: Forum-Porn\n",
      "scraping: Forum-VPN\n",
      "scraping: Forum-Shopping-Deals-Discounts\n",
      "scraping: Forum-Source-codes\n",
      "scraping: Forum-Porn-leaks\n",
      "scraping: Forum-Other-Leaks\n",
      "scraping: Forum-Requests\n",
      "scraping: Forum-Monetizing-Techniques\n",
      "scraping: Forum-E-Whoring\n",
      "scraping: Forum-Social-Engineering\n",
      "scraping: Forum-Tutorials\n",
      "scraping: Forum-Resources\n",
      "scraping: Forum-Cryptocoins\n",
      "scraping: Forum-Discussion\n",
      "scraping: Forum-Show-off\n",
      "scraping: Forum-Tips\n",
      "scraping: Forum-Rumors\n",
      "scraping: Forum-Real-life-businesses\n",
      "scraping: Forum-Programming-discussions\n",
      "scraping: Forum-Webhosting-discussion\n",
      "scraping: Forum-C-C-VB-NET\n",
      "scraping: Forum-Java\n",
      "scraping: Forum-Web-Programming\n",
      "scraping: Forum-Other-languages\n",
      "scraping: Forum-Python\n",
      "scraping: Forum-Marketplace-discussions\n",
      "scraping: Forum-Deal-disputes\n",
      "scraping: Forum-Archive--204\n",
      "scraping: Forum-Sellers-Marketplace\n",
      "scraping: Forum-Products\n",
      "scraping: Forum-Services\n",
      "scraping: Forum-Refunding-Services\n",
      "scraping: Forum-Accounts--122\n",
      "scraping: Forum-E-Books-Guides\n",
      "scraping: Forum-Exchange\n",
      "scraping: Forum-Buyers-bay\n",
      "scraping: Forum-Services--289\n",
      "scraping: Forum-Accounts--290\n",
      "scraping: Forum-Hiring\n",
      "scraping: Forum-Credits-Trading\n",
      "scraping: Forum-Marketplace-Archive\n",
      "scraping: Forum-Premium-Sellers-Archive\n",
      "scraping: Forum-Normal-Sellers-Archive\n",
      "scraping: Forum-Buyers-Bay-Archive\n",
      "scraping: Forum-Premium-Accounts\n",
      "scraping: Forum-Request--272\n",
      "scraping: Forum-Gaming--104\n",
      "scraping: Forum-Streaming\n",
      "scraping: Forum-Porn--106\n",
      "scraping: Forum-Premium-Leaks\n",
      "scraping: Forum-Premium-Lounge\n",
      "scraping: https://cracked.io/Forum-Announcements\n",
      "scraping: https://cracked.Io/Forum-Announcements\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('cracked.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''select url from urls''')\n",
    "urls = cursor.fetchall()\n",
    "\n",
    "for url in urls:\n",
    "    scrape_topics(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_posts(url):\n",
    "    print(f\"scraping: {url[0]}\")\n",
    "    archive_url = f\"https://cracked.io/{url[0]}\"\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(archive_url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all(\"div\", class_=\"post-box\")\n",
    "    for post in posts:\n",
    "        try:\n",
    "            #print(post)\n",
    "            user_id = post.find(\"div\", class_=\"post-username\").find(\"a\").get(\"href\").split(\"/\")[-1]\n",
    "            #print(f\"user_id: {user_id}\")\n",
    "            author = post.find(\"div\", class_=\"post-username\").text.strip()\n",
    "            #print(f\"author: {author}\")\n",
    "            body = post.find(\"div\", class_=\"post_body\").text.strip()\n",
    "            #print(f\"body: {body}\")\n",
    "            timestamp = post.find(\"span\", class_=\"post_date\").text.strip()\n",
    "            #print(f\"timestamp: {timestamp}\")\n",
    "            # Apply to sqlite\n",
    "            cursor.execute(\"INSERT or ignore INTO posts VALUES (?,?,?,?,?,?)\",(url[0],url[1],user_id,author, body,timestamp))\n",
    "            conn.commit()\n",
    "            print(f\"Inserted: {url[1]} {user_id}\")\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping: Thread-FREE-ROBLOX-COOKIE\n",
      "scraping: Thread-HOW-TO-CRACK-ACCOUNTS-WITH-%E2%9C%94%EF%B8%8F%E2%9C%94%EF%B8%8FOPENBULLET%E2%9C%94%EF%B8%8F%E2%9C%94%EF%B8%8F-FOR-BEGINNERS\n",
      "scraping: Thread-%E2%9C%85%EF%B8%8F-FREE-MASTER-DOXXING-AND-OSINT-TOOL-WITH-GUI-ANYONE-CAN-DOX-EVEN-YOU-%E2%9C%85%EF%B8%8F--1155557\n",
      "scraping: Thread-roblox-cookie-with-limiteds\n",
      "scraping: Thread-Supreme-Join-discord-gg-kanker\n",
      "scraping: Thread-%E2%AD%90LEAKED-2023%E2%AD%90%E2%9C%85-ONLYFANS-CHECKER%E2%9C%85-%E2%AD%90FULL-CAPTURE%E2%AD%90%E2%9C%85-100-WORKING%E2%9C%85\n",
      "scraping: Thread-NEW-IPTV-CONFIG-2024-FULL-CAPTURE-ALL-SERVERS-NO-PROXIES-NEDDED-MAC-PASS\n",
      "scraping: Thread-HUGE-COLLECTION-OF-UDEMY-COURSES%E2%AD%90-UDEMY-LEAK-%E2%AD%90-Different-Courses-Packed-in-One\n",
      "scraping: Thread-X138-PREMIUM-DISNEY-HITS\n",
      "scraping: Thread-Streaming-%E2%AD%903400X-NETFLIX-ACCOUNTS%E2%AD%90FULL-URL-CAPTURE-PREMIUM%E2%AD%90\n",
      "scraping: Thread-Streaming-%E2%9A%A1-HBO-MAX-%E2%9A%A1-%E2%9A%A1X34-HBOPREMIUM-HITS%E2%AD%90FRESH-HQ%E2%AD%90-100-WORKS-%E2%AD%90\n",
      "scraping: Thread-Streaming-%E2%AD%90-DIRECT-TV-%E2%AD%90%E2%9A%A1-18X-DIRECT-TV-HITS%E2%9A%A1PREMIUM-STREAMING%E2%9A%A1%E2%AD%90-FRESH-HITS-%E2%AD%90--1159730\n",
      "scraping: Thread-Netflix-%E2%93%82%EF%B8%8F-NETFLIX-%E2%9D%A3%EF%B8%8F-5x-NETFLIX-PREMIUM-UHD-%E2%9D%A3%EF%B8%8F%E2%98%84%EF%B8%8F-FULL-CAPTURE-%E2%98%84%EF%B8%8F%E2%9D%A3%EF%B8%8F-NETFLIX-PREMIUM-%E2%93%82%EF%B8%8F\n",
      "scraping: Thread-KELLYMADISONMEDIA-%E2%AD%90PORNFIDELITYNETWORK-%E2%AD%90TEENFIDELITY-%E2%AD%90--1159708\n",
      "scraping: Thread-Familystrokes-com-Family-Strokes-Taboo-Family-Porn-Taboo-Family-Sex-%E2%AD%90--1159706\n",
      "scraping: Thread-BRAZZERS-COM-x-5-PREMIUM-100-WORKING-ACCOUNT--1159714\n",
      "scraping: Thread-BLACKED-BLACKEDRAW-TUSHY-TUSHYRAW-DEEPER-SLAYED-PORTAL--1159712\n",
      "scraping: Thread-%E2%9C%A8EXPRESSVPN%E2%9C%A8-%E2%9C%85x22-ACCOUNTS-%E2%9C%85\n",
      "scraping: Thread-%E2%AD%90X26-NORDVPN-LOGS-working-when-posted-%E2%AD%90-VERIFIED-SOURCE\n",
      "scraping: Thread-x14-EXPRESSVPN-ACCOUNTS-%E2%9D%8C-CAPTURE-INCLUDED-%E2%9D%8C\n",
      "scraping: Thread-Porn-RANDOM-TEEN-NUDES-LEAKED-HOMEMADE-AMATEUR-LEAKED-FROM-SNAPCHAT-AND-ICLOUD-HACKS--1159101\n",
      "scraping: Thread-Porn-PREMIUM-TEEN-DROPBOX-LEAKS--1159725\n",
      "scraping: Thread-Porn-%E2%9D%A4%EF%B8%8F-MEGA-%E2%9D%A4%EF%B8%8F-12-NEW-SNAPL3K-SNLK422-%E2%9D%A4%EF%B8%8F--1159722\n",
      "scraping: Thread-Porn-PRIVATE-LEAK-500GB-OF-AMATEUR-LEAKS-%E2%9C%85-LOTS-OF-CONTENT-%E2%9C%85-MAY-2023-LEAKS-%E2%9C%85--1159721\n",
      "scraping: Thread-Porn-%E2%9D%A4%EF%B8%8F-MEGA-%E2%9D%A4%EF%B8%8F-ONLYFANS-LEAKS-PACK-3-05-TB-MEGA-NZ-%E2%9D%A4%EF%B8%8F\n",
      "scraping: Thread-Porn-%E2%AD%90-MEGA-%E2%AD%90-%E2%9D%A4%EF%B8%8F-VOYEUR-SPY-HIDDEN-CAMERA-%E2%9D%A4%EF%B8%8F-%E2%9A%9C%EF%B8%8FBIG-COLLECTIONS-%E2%9A%9C%EF%B8%8F--1159726\n",
      "scraping: Thread-Porn-%E2%98%84%EF%B8%8FSMALL-HOT-YOUNG%E2%9D%A4%EF%B8%8F%E2%98%84%EF%B8%8F-TEEN-18-%E2%AD%90LEAKED-TEEN-NUDES--1159302\n",
      "scraping: Thread-%E2%9C%A8-PRIVATE-METHOD-%E2%9C%A8-HOW-TO-BOOK-HOTELS-FOR-FREE-WITHOUT-CARDING-OR-REFUNDING-INVOLVED\n",
      "scraping: Thread-%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F400-Udemy-Courses-%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8F%E2%AD%90%EF%B8%8FCOMPLETELY-FREE-FREE\n",
      "scraping: Thread-Netflix-%E2%9D%A4%EF%B8%8F%E2%AD%90Free-Site-To-Get-Free-Netflix-Premium-Upgrade-100-Working%E2%9D%A4%EF%B8%8F%E2%AD%90--1163713\n",
      "scraping: Thread-BLACKED-BLACKEDRAW-TUSHY-TUSHYRAW-DEEPER-SLAYED-PORTAL--1163662\n",
      "scraping: Thread-%E2%9C%A8PORNPROS-ACCOUNT%E2%9C%A8-%E2%9C%A8OVER-10-SITES%E2%9C%A8--1163661\n",
      "scraping: Thread-Buy-an-invalid-combination--1163706\n",
      "scraping: Thread-buy-Upgrade-to-Premium-cracked-VIP-USDT-payment\n",
      "scraping: Thread-looking-for-spotify-upgrade-family-link-for-cheap\n",
      "scraping: Thread-wts-140-000-credits\n",
      "scraping: Thread-BreachForums-Returns-Under-the-Control-of-ShinyHunters-Hackers\n",
      "scraping: Thread-%E2%9C%A8X1000-VALORANT-ACCOUNTS%E2%9C%A8FRESH-CRACKED%E2%9C%A8SKINS-RANK-USER-INFO%E2%9C%A8--1180915\n",
      "scraping: Thread-%E2%AD%90TEAMSKEET-FULL-ACCESS%E2%AD%90-%E2%AD%90OVER-65-SITES--1181002\n",
      "scraping: Thread-BLACKED-BLACKEDRAW-TUSHY-TUSHYRAW-DEEPER-SLAYED-PORTAL--1181005\n",
      "scraping: Thread-%E2%AD%90PORNPORTAL-PREMIUM-ACCOUNTS-%E2%AD%90--1180994\n",
      "scraping: Thread-%E2%9C%A8PORNPROS-ACCOUNT%E2%9C%A8-%E2%9C%A8OVER-10-SITES%E2%9C%A8--1181003\n",
      "scraping: Thread-BRAZZERS-COM-x-5-PREMIUM-100-WORKING-ACCOUNT--1181007\n",
      "scraping: Thread-soon--1180715\n"
     ]
    }
   ],
   "source": [
    "conn = sqlite3.connect('cracked.db')\n",
    "cursor = conn.cursor()\n",
    "\n",
    "cursor.execute('''select topics.url, topics.title from topics left join posts on posts.url = topics.url where posts.url is NULL''')\n",
    "topics = cursor.fetchall()\n",
    "\n",
    "for topic in topics:\n",
    "    scrape_posts(topic)\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
