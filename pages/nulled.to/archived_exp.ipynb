{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import random\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect('nulled.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute('''CREATE TABLE IF NOT EXISTS archive_urls\n",
    "            (original TEXT, mimetype TEXT, timestamp TEXT, endtimestamp TEXT, groupcount integer, uniqcount integer)''')\n",
    "\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "\n",
    "# Load the JSON data\n",
    "with open(r'C:\\Users\\Aston\\Documents\\GitHub\\web_scraper\\nulled.to\\archive.json', 'r') as file:\n",
    "    json_data = json.load(file)\n",
    "\n",
    "# Insert the data into the SQLite table\n",
    "for row in json_data:\n",
    "    cursor.execute('INSERT INTO archive_urls VALUES (?, ?, ?, ?, ?, ?)', row)\n",
    "\n",
    "conn.commit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_agents = [ \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36', \n",
    "    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36', \n",
    "    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/90.0.4430.212 Safari/537.36', \n",
    "    'Mozilla/5.0 (iPhone; CPU iPhone OS 12_2 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Mobile/15E148', \n",
    "    'Mozilla/5.0 (Linux; Android 11; SM-G960U) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.72 Mobile Safari/537.36' \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tables(c):\n",
    "    cursor = c.cursor()\n",
    "    # Create a table if it doesn't exist\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS users\n",
    "                (username TEXT,location text)''')\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS topics\n",
    "                (url TEXT,title text,author text,timestamp text,replies text,views text,last_post text)''')\n",
    "    c.commit()\n",
    "create_tables(conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic(url):\n",
    "    print(f\"scraping: {url[0]}\")\n",
    "    archive_url = f\"http://web.archive.org/web/{url[0]}\"\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(archive_url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    topics = soup.find_all(\"tr\",class_=\"__topic\")\n",
    "    for topic in topics:\n",
    "        try:\n",
    "            elements = topic.find_all(\"td\")\n",
    "            topic_url = elements[1].find(\"a\").get(\"href\")\n",
    "            title = elements[1].find(\"a\").get(\"title\").split(\"-\")[0].strip()\n",
    "            timestamp = \" \".join(elements[1].find(\"a\").get(\"title\").split(\"-\")[1].split(\" \")[3:])\n",
    "\n",
    "            author = elements[1].find(class_=\"reverser\").text.strip()\n",
    "\n",
    "            replied = elements[2].text.strip().split(\"\\n\")[1]\n",
    "\n",
    "            views = elements[3].text.strip().split(\"\\n\")[0]\n",
    "\n",
    "            last_post = elements[-1].text.strip().split(\"\\n\")[0]\n",
    "\n",
    "            cursor.execute(\"INSERT or ignore INTO topics VALUES (?,?, ?, ?, ?, ?, ?)\",(topic_url,title, author, timestamp, replied,views,last_post))\n",
    "            conn.commit()\n",
    "            cursor.execute(\"INSERT INTO scraped_urls VALUES (?)\",(url[1],))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_topic([\"20190408110906/https://www.nulled.to/forum/3-the-lounge/\",\"test\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_banlist(url):\n",
    "    print(f\"scraping: {url[0]}\")\n",
    "    archive_url = f\"http://web.archive.org/web/{url[0]}\"\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(archive_url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    ban_element = soup.find(\"tbody\",class_='ban-tbody')\n",
    "    ban_list = soup.find_all(\"tr\")\n",
    "    for ban in ban_list:\n",
    "        try:\n",
    "            elements = ban.find_all(\"td\")\n",
    "            username = elements[0].text.strip()\n",
    "            # get image\n",
    "            location = ban.find(\"img\").get(\"src\").split(\"/\")[-1]\n",
    "\n",
    "            cursor.execute(\"INSERT or ignore INTO users VALUES (?,?)\",(username,location))\n",
    "            conn.commit()\n",
    "            cursor.execute(\"INSERT INTO scraped_urls VALUES (?)\",(url[1],))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(e):\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping: 20220607002910/https://www.nulled.to/ban-list.php?&st=0\n"
     ]
    }
   ],
   "source": [
    "scrape_banlist([\"20220607002910/https://www.nulled.to/ban-list.php?&st=0\",\"test\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_award(url):\n",
    "    print(f\"scraping: {url[0]}\")\n",
    "    archive_url = f\"http://web.archive.org/web/{url[0]}\"\n",
    "    user_agent = random.choice(user_agents) \n",
    "    headers = {'User-Agent': user_agent}\n",
    "    response = requests.get(archive_url,headers=headers)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    list = soup.find_all(\"tr\")\n",
    "    for row in list:\n",
    "        try:\n",
    "            elements = row.find_all(\"td\")\n",
    "            username = elements[0].text.strip()\n",
    "            cursor.execute(\"INSERT or ignore INTO users VALUES (?)\",(username))\n",
    "            conn.commit()\n",
    "            cursor.execute(\"INSERT INTO scraped_urls VALUES (?)\",(url[1],))\n",
    "            conn.commit()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_award([\"20220124055516/https://www.nulled.to/awards.php?award_id=12\",\"test\"]) #works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute('''select endtimestamp ||\"/\" || original as url, original  from archive_urls left join scraped_urls on scraped_urls.url = original where mimetype = \"text/html\" and scraped_urls.url is Null ''')\n",
    "urls = cursor.fetchall()\n",
    "for url in urls:\n",
    "    # Check if the url string constain the word forum\n",
    "    if '/forum/' in url[0].lower():\n",
    "        scrape_topic(url)\n",
    "\n",
    "        time.sleep(1)\n",
    "    elif 'ban-list.php' in url[0].lower():\n",
    "        scrape_banlist(url)\n",
    "        time.sleep(1)\n",
    "    elif 'awards.php' in url[0].lower():\n",
    "        scrape_award(url)\n",
    "        time.sleep(1)\n",
    "    else:\n",
    "        continue\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
